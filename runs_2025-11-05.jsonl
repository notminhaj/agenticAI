{"title": "Goldfish: Monolingual Language Models for 350 Languages", "url": "https://arxiv.org/abs/2408.10441v1", "summary": "**Problem:** Many low-resource languages rely on large multilingual models that often perform poorly compared to simpler models, such as bigrams. Current models struggle to provide adequate performance in generating text for a significant number of these languages.\n\n**Approach:** The authors developed and released Goldfish, a suite of monolingual autoregressive Transformer language models with up to 125M parameters, specifically designed for 350 languages. They benchmarked these models against existing multilingual models, focusing on FLORES perplexity metrics.\n\n**Key Results:** Goldfish models achieve lower FLORES perplexities than larger multilingual models for 98 out of 204 languages tested, despite being over 10 times smaller. However, they still lag behind in reasoning benchmarks, indicating a trade-off in capabilities.\n\n**Why It Matters:** Goldfish offers a valuable resource for low-resource language research, providing baseline models and fine-tuning options. This advancement promotes better performance in NLP tasks for these languages and enhances cross-linguistic studies with comparable models.", "tokens_in": 383, "tokens_out": 201}\n{"title": "Lost in Translation: Large Language Models in Non-English Content Analysis", "url": "https://arxiv.org/abs/2306.07377v1", "summary": "**Problem:** Large language models predominantly excel in English, creating a disparity in their performance across the world's 7,000 languages. This limits the effectiveness of AI systems like chatbots and content moderation tools in non-English contexts.\n\n**Approach:** The paper discusses multilingual language models that aim to address the limitations of traditional models by enabling better language analysis beyond English. It explains the technical workings of these models and the challenges they face in content analysis.\n\n**Key Results:** The authors outline the capabilities and limitations of multilingual language models, highlighting the data gap between English and other languages. They also discuss the general challenges of content analysis using both large and multilingual models.\n\n**Why It Matters:** Understanding the effectiveness and shortcomings of multilingual language models is crucial for companies, researchers, and policymakers. It informs better practices for developing AI systems that can operate fairly and efficiently across diverse languages.", "tokens_in": 331, "tokens_out": 173}\n{"title": "How Good are Commercial Large Language Models on African Languages?", "url": "https://arxiv.org/abs/2305.06530v1", "summary": "**Problem:** The performance of commercial large language models on African languages remains largely unexplored, despite their widespread use in Natural Language Processing (NLP) applications. There is a concern about their effectiveness in tasks like machine translation and text classification for these languages.\n\n**Approach:** The study conducts a preliminary analysis of commercial large language models by evaluating their performance on machine translation and text classification tasks across eight African languages from various language families and regions. \n\n**Key Results:** Results indicate that commercial language models underperform on African languages, particularly in machine translation, while showing relatively better results in text classification tasks. Overall, the performance is deemed below par compared to expectations.\n\n**Why It Matters:** This research highlights the need for improved representation of African languages in commercial language models, emphasizing the importance of inclusivity in AI and NLP technologies as they continue to gain traction globally.", "tokens_in": 265, "tokens_out": 172}\n{"title": "Modelling Language", "url": "https://arxiv.org/abs/2404.09579v1", "summary": "**Problem:** The paper addresses the underappreciation of large language models (LLMs) in linguistic studies, arguing that they can provide valuable insights about language as a social entity, beyond cognitive processes related to linguistic competence.\n\n**Approach:** The authors defend the scientific utility of LLMs by countering arguments that they lack linguistic insight. They draw on recent philosophy of science to illustrate how LLMs can function effectively as models of language.\n\n**Key Results:** The paper demonstrates that LLMs can contribute to linguistic research by modeling language use in social contexts, highlighting their potential to enhance understanding of language as a dynamic, external phenomenon.\n\n**Why It Matters:** Recognizing LLMs as scientific models can reshape linguistic research, fostering a more holistic view of language that encompasses both cognitive and social dimensions, ultimately advancing the field of linguistics.", "tokens_in": 203, "tokens_out": 170}\n